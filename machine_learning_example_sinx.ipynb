{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_example_nn_sinx.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTWUZd1xkM6z",
        "colab_type": "text"
      },
      "source": [
        "# Example #1: Neural Network for $y = \\sin(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGyCRUztkUtH",
        "colab_type": "text"
      },
      "source": [
        "Same example as yesterday, a sine-curve with 10 points as training values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HztJmQrdtP8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(0,6.6, 0.6)\n",
        "y = np.sin(x)\n",
        "\n",
        "xplot = np.arange(0, 6.6, 0.01)\n",
        "yplot = np.sin(xplot)\n",
        "\n",
        "plt.scatter(x,y, color=\"b\", label=\"Training\")\n",
        "plt.plot(xplot, yplot, color=\"g\", label=\"sin(x)\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEhhYqOMlAuz",
        "colab_type": "text"
      },
      "source": [
        "## Defining the architecture of our neural network:\n",
        "\n",
        "Fully connected with 1 input node, 1 hidden layer, 1 output node.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUlnr_Y4j2Iw",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/v27q53W.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJAUYmTblUGO",
        "colab_type": "text"
      },
      "source": [
        "Layer connections:\n",
        "\\begin{equation}\n",
        "y = b+\\sum_i x_i w_i\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "**Question:** \"How many weights are there in the above example?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsWxNiMvluza",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Activation function (sigmoid):\n",
        "\\begin{equation}\n",
        "\\sigma\\left(x\\right) = \\frac{1}{1 + \\exp\\left(-x\\right)}\n",
        "\\end{equation}\n",
        "Popular because the derivative of the sigmoid function is simple:\n",
        "\\begin{equation}\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d}x}\\sigma\\left(x\\right) = \\sigma\\left(x\\right)\\left(1 - \\sigma\\left(x\\right)\\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg8xLxUq_Ib3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(val):\n",
        "\n",
        "  sigmoid = 1.0 / (1.0 + np.exp(-val))\n",
        "  return sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZCIjICEm3Xn",
        "colab_type": "text"
      },
      "source": [
        "### Defining the architecture (i.e. the layers):\n",
        "\n",
        "*   `input_value` - Input value\n",
        "*   `w_ih` - Weights that connect input layer with hidden layer\n",
        "*   `w_io` - Weights that connect hidden layer with output layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EZ8gofBNDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(input_value, w_ih, w_ho):\n",
        "\n",
        "  hidden_layer = activation(input_value * w_ih)\n",
        "  output_value = np.sum(hidden_layer*w_ho)\n",
        "\n",
        "  return output_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIUX7BjGoBmh",
        "colab_type": "text"
      },
      "source": [
        "Let's start by testing the neural network with random weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5MrGCtAA6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1000)\n",
        "random_weights_ih = np.random.random(10)\n",
        "random_weights_ho = np.random.random(10)\n",
        "\n",
        "print(random_weights_ih)\n",
        "print(random_weights_ho)\n",
        "print()\n",
        "\n",
        "val = 2.0\n",
        "sinx_predicted = model(val, random_weights_ih, random_weights_ho)\n",
        "\n",
        "print(\"Predicted:\", sinx_predicted)\n",
        "print(\"True:     \", np.sin(2.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBDc4eUkm1uZ",
        "colab_type": "text"
      },
      "source": [
        "Setting our Model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIU3_4C1HUU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The number of nodes in the hidden layer\n",
        "HIDDEN_LAYER_SIZE = 40\n",
        "\n",
        "# L2-norm regularization\n",
        "L2REG = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-JDaotgskIK",
        "colab_type": "text"
      },
      "source": [
        "## Optimizing the weights:\n",
        "\n",
        "We want to find the best set of weights $\\mathbf{w}$ that minimizes some loss function. For example we can minimize the squared error (like we did in least squares fitting):\n",
        "\n",
        "\\begin{equation}\n",
        "L\\left(\\mathbf{w}\\right) = \\sum_i \\left(y_i^\\mathrm{true} -  y_i^\\mathrm{predicted}(\\mathbf{w}) \\right)^{2}\n",
        "\\end{equation}\n",
        "Or with L2-regularization:\n",
        "\\begin{equation}\n",
        "L\\left(\\mathbf{w}\\right) = \\sum_i \\left(y_i^\\mathrm{true} -  y_i^\\mathrm{predicted}(\\mathbf{w}) \\right)^{2} + \\lambda\\sum_j w_j^{2}\n",
        "\\end{equation}\n",
        "Just like in the numerics lectures and exercises, we can use a function from SciPy to do this minimization: `scipy.optimize.minimize()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSyj7GADmjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(parameters):\n",
        "\n",
        "  w_ih = parameters[:HIDDEN_LAYER_SIZE]\n",
        "  w_ho = parameters[HIDDEN_LAYER_SIZE:]\n",
        "\n",
        "  squared_error = 0.0\n",
        "\n",
        "  for i in range(len(x)):\n",
        "\n",
        "    # Predict y for x[i]\n",
        "    y_predicted = model(x[i], w_ih, w_ho)\n",
        "    \n",
        "    # Without # Regularization\n",
        "    squared_error = squared_error + (y[i] - y_predicted)**2 \n",
        "    \n",
        "    # With regularization\n",
        "    # rmse += (z - y[i])**2 + np.linalg.norm(parameters) * L2REG\n",
        "       \n",
        "  return squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R5qtIyiwnGQ",
        "colab_type": "text"
      },
      "source": [
        "## Running the minimization with `scipy.optimize.minimize()`:\n",
        "\n",
        "Documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
        "\n",
        "Since we haven't implemented the gradient of the neural network, we can't use optimizers that require the gradient. One algorithm we can use is the Nelder-Mead optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwqtWfWFDIAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define random initial weights\n",
        "np.random.seed(666)\n",
        "p = np.random.random(size=2*HIDDEN_LAYER_SIZE)\n",
        "\n",
        "# Minimize the loss function with parameters p\n",
        "result = minimize(loss_function, p, method=\"Nelder-Mead\",\n",
        "                  options={\"maxiter\": 100000, \"disp\": True})\n",
        "\n",
        "wfinal_in = result.x[:HIDDEN_LAYER_SIZE]\n",
        "wfinal_hl = result.x[HIDDEN_LAYER_SIZE:]\n",
        "\n",
        "print(wfinal_in)\n",
        "print(wfinal_hl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmNKGdUHxGQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Print sin(2.5) and model(2.5)\n",
        "val = 2.5\n",
        "sinx_predicted = model(val, wfinal_in, wfinal_hl)\n",
        "\n",
        "print(\"Predicted:\", sinx_predicted)\n",
        "print(\"True:     \", np.sin(val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbnF2nsjj8M",
        "colab_type": "text"
      },
      "source": [
        "Lets make a plot with pyplot!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_2BohmuE_gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xplot = np.arange(0,6.6, 0.01)\n",
        "yplot = np.sin(xplot)\n",
        "\n",
        "ypred = np.array([model(val, wfinal_in, wfinal_hl) for val in xplot])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(xplot,yplot, color=\"g\", label=\"sin(x)\")\n",
        "plt.scatter(x, y, color=\"b\", label=\"Training\")\n",
        "plt.plot(xplot, ypred, color=\"r\", label=\"Predicted\")\n",
        "\n",
        "plt.ylim([-2,2])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lqeNx4byDKE",
        "colab_type": "text"
      },
      "source": [
        "## What to do about \"crazy\" behaviour?\n",
        "*   Regularization\n",
        "*   Adjust hyperparameters (hidden layer size)"
      ]
    }
  ]
}